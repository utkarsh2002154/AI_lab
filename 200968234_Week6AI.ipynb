{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Importing the libraries and loading dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "ads_clicks = pd.read_csv('/content/Ads_Optimisation.csv')"
      ],
      "metadata": {
        "id": "0rkwk4WIDyEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of ads\n",
        "num_ads = len(ads_clicks.columns)\n",
        "print(num_ads)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4fFqlsmE80o",
        "outputId": "f502afa1-c7b9-41c3-ed34-bbf20a050cc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ε-greedy algorithm\n",
        "def epsilon_greedy(epsilon, rewards):\n",
        "    if random.uniform(0, 1) < epsilon:\n",
        "        # Explore: Choose a random ad\n",
        "        ad = random.randint(0, num_ads - 1)\n",
        "    else:\n",
        "        # Exploit: Choose the ad with the highest reward\n",
        "        ad = np.argmax(rewards)\n",
        "    return ad"
      ],
      "metadata": {
        "id": "v3fVLdk5Ltvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the rewards for each ad to 0 and create an empty list to store the rewards for each time step:\n",
        "rewards = [0] * num_ads\n",
        "total_rewards_01 = []\n",
        "total_rewards_03 = []"
      ],
      "metadata": {
        "id": "D8uusnFeLx6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterating the ε-greedy algorithm for 2000 time steps using ε=0.01 and ε=0.3\n",
        "\n",
        "for t in range(2000):\n",
        "\n",
        "    # Choosing ad using the epsilon-greedy algorithm with epsilon=0.01\n",
        "    ad_01 = epsilon_greedy(0.01, rewards)\n",
        "\n",
        "    # Choose the ad using the epsilon-greedy algorithm with epsilon=0.3\n",
        "    ad_03 = epsilon_greedy(0.3, rewards)\n",
        "    \n",
        "    # for epsilon = 0.01\n",
        "    # reward for the chosen ad\n",
        "    reward = ads_clicks.iloc[t][ad_01]\n",
        "    # Updating rewards for the chosen ad\n",
        "    rewards[ad_01] = rewards[ad_01] + reward\n",
        "    # Add the reward to the total rewards list for epsilon=0.01\n",
        "    total_rewards_01.append(sum(rewards))\n",
        "    \n",
        "    # for epsilon = 0.3\n",
        "    # reward for the chosen ad\n",
        "    reward = ads_clicks.iloc[t][ad_03]\n",
        "    # Updating rewards for the chosen ad\n",
        "    rewards[ad_03] = rewards[ad_03] + reward\n",
        "    # Add the reward to the total rewards list for epsilon=0.3\n",
        "    total_rewards_03.append(sum(rewards))"
      ],
      "metadata": {
        "id": "M6Y1Wo5xMMAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total rewards for ε=0.01: \", total_rewards_01[-1])\n",
        "print(\"Total rewards for ε=0.3: \", total_rewards_03[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wb06mc1mMPy_",
        "outputId": "67f9483d-2d99-4c0d-b3e4-b5348f46bba1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rewards for ε=0.01:  659\n",
            "Total rewards for ε=0.3:  660\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upper-Confidence-Bound algorithm\n",
        "def ucb(rewards, n, t, c=1.5):\n",
        "    # Calculate the average reward for each ad\n",
        "    average_rewards = rewards / n\n",
        "    # Calculate the upper confidence bound for each ad\n",
        "    ucb_values = average_rewards + c * np.sqrt(np.log(t + 1) / n)\n",
        "    # Choose the ad with the highest UCB value\n",
        "    ad = np.argmax(ucb_values)\n",
        "    return ad"
      ],
      "metadata": {
        "id": "Peu_lxpORHAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the rewards for each ad to 0 and create an empty list to store the rewards for each time step:\n",
        "rewards = np.zeros(num_ads)\n",
        "n = np.zeros(num_ads)\n",
        "total_rewards = []"
      ],
      "metadata": {
        "id": "4HYRi5R9U59r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterating over Upper-Confidence-Bound algorithm for 2000 time steps using c=1.5:\n",
        "for t in range(2000):\n",
        "    # Choose the ad using the UCB algorithm\n",
        "    ad = ucb(rewards, n, t, c=1.5)\n",
        "    \n",
        "    # Get the reward for the chosen ad\n",
        "    reward = ads_clicks.iloc[t][ad]\n",
        "    # Update the rewards for the chosen ad\n",
        "    rewards[ad] = rewards[ad] + reward\n",
        "    # Update the number of times the ad has been selected\n",
        "    n[ad] = n[ad] + 1\n",
        "    # Add the reward to the total rewards list\n",
        "    total_rewards.append(sum(rewards))\n",
        "\n",
        "# Print the total rewards for c=1.5\n",
        "print(\"Total rewards for c=1.5: \", total_rewards[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8ZugbLbU-tE",
        "outputId": "eb53c07b-9e47-4c39-a610-8ed91f29a0c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rewards for c=1.5:  763.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### How the action value estimated compares to the optimal action in the ε-greedy approach :\n",
        "\n",
        "In the ε-greedy approach, action value is estimated using the **sample average method**, where the \"**estimated value of each action = average of the rewards received for that action**\". \n",
        "\n",
        "However, the sample average method can give large variance in the estimate if the number of samples is small. Thereby, the estimates may not converge to the true action values quickly. Hence, the ε-greedy approach tends to explore more at the beginning of the experiment to reduce uncertainty, and then exploit the best action based on the estimated action values later on.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### How the action value estimated compares to the optimal action in the UCB approach :\n",
        "\n",
        "The Upper-Confidence-Bound approach, on the other hand, estimates the action values using the **Upper Confidence Bound (UCB)**, which is a **combination of the average reward and an uncertainty term**. \n",
        "\n",
        "The uncertainty term ensures that actions that have not been selected many times are given a higher priority to be selected, while actions that have been selected many times are given a lower priority. This approach results in a more stable estimate of the action values, and the algorithm converges more quickly to the optimal action.\n",
        "\n"
      ],
      "metadata": {
        "id": "sSTFSOD7XvjU"
      }
    }
  ]
}