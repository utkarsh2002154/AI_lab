{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Learn  the  optimal  policy  for  the  frozen  lake environment  using  the  Policy  Iteration  vs  the Value Iteration technique."
      ],
      "metadata": {
        "id": "TNlFFAdsFc4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install gym"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqaYvpbZRhJM",
        "outputId": "d31379e0-579d-4c06-e491-d53bd290b67d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.9/dist-packages (0.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.9/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from gym) (6.1.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from gym) (1.22.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8.0->gym) (3.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "c0AdFXZsE6AH"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gym import envs\n",
        "print(envs.registry.all())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EAJ3ZsWRn1F",
        "outputId": "34635dfe-e1a1-40b1-9c31-618c0a1092da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_values([EnvSpec(id='CartPole-v0', entry_point='gym.envs.classic_control.cartpole:CartPoleEnv', reward_threshold=195.0, nondeterministic=False, max_episode_steps=200, order_enforce=True, autoreset=False, disable_env_checker=False, new_step_api=False, kwargs={}, namespace=None, name='CartPole', version=0), EnvSpec(id='CartPole-v1', entry_point='gym.envs.classic_control.cartpole:CartPoleEnv', reward_threshold=475.0, nondeterministic=False, max_episode_steps=500, order_enforce=True, autoreset=False, disable_env_checker=False, new_step_api=False, kwargs={}, namespace=None, name='CartPole', version=1), EnvSpec(id='MountainCar-v0', entry_point='gym.envs.classic_control.mountain_car:MountainCarEnv', reward_threshold=-110.0, nondeterministic=False, max_episode_steps=200, order_enforce=True, autoreset=False, disable_env_checker=False, new_step_api=False, kwargs={}, namespace=None, name='MountainCar', version=0), EnvSpec(id='MountainCarContinuous-v0', entry_point='gym.envs.classic_control.continuous_mountain_car:Continuous_MountainCarEnv', reward_threshold=90.0, nondeterministic=False, max_episode_steps=999, order_enforce=True, autoreset=False, disable_env_checker=False, new_step_api=False, kwargs={}, namespace=None, name='MountainCarContinuous', version=0), EnvSpec(id='Pendulum-v1', entry_point='gym.envs.classic_control.pendulum:PendulumEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=200, order_enforce=True, autoreset=False, disable_env_checker=False, new_step_api=False, kwargs={}, namespace=None, name='Pendulum', version=1), EnvSpec(id='Acrobot-v1', entry_point='gym.envs.classic_control.acrobot:AcrobotEnv', reward_threshold=-100.0, nondeterministic=False, max_episode_steps=500, order_enforce=True, autoreset=False, disable_env_checker=False, new_step_api=False, kwargs={}, namespace=None, name='Acrobot', version=1), EnvSpec(id='LunarLander-v2', entry_point='gym.envs.box2d.lunar_lander:LunarLander', reward_threshold=200, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, new_step_api=False, kwargs={}, namespace=None, name='LunarLander', version=2), EnvSpec(id='LunarLanderContinuous-v2', entry_point='gym.envs.box2d.lunar_lander:LunarLander', reward_threshold=200, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, new_step_api=False, kwargs={'continuous': True}, namespace=None, name='LunarLanderContinuous', version=2), EnvSpec(id='BipedalWalker-v3', entry_point='gym.envs.box2d.bipedal_walker:BipedalWalker', reward_threshold=300, nondeterministic=False, max_episode_steps=1600, order_enforce=True, autoreset=False, disable_env_checker=False, new_step_api=False, kwargs={}, namespace=None, name='BipedalWalker', version=3), EnvSpec(id='BipedalWalkerHardcore-v3', entry_point='gym.envs.box2d.bipedal_walker:BipedalWalker', reward_threshold=300, nondeterministic=False, max_episode_steps=2000, order_enforce=True, autoreset=False, disable_env_checker=False, new_step_api=False, kwargs={'hardcore': True}, namespace=None, name='BipedalWalkerHardcore', version=3), EnvSpec(id='CarRacing-v2', entry_point='gym.envs.box2d.car_racing:CarRacing', reward_threshold=900, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, new_step_api=False, kwargs={}, namespace=None, name='CarRacing', version=2), EnvSpec(id='Blackjack-v1', entry_point='gym.envs.toy_text.blackjack:BlackjackEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=True, autoreset=False, disable_env_checker=False, new_step_api=False, kwargs={'sab': True, 'natural': False}, namespace=None, name='Blackjack', version=1), EnvSpec(id='FrozenLake-v1', entry_point='gym.envs.toy_text.frozen_lake:FrozenLakeEnv', reward_threshold=0.7, nondeterministic=False, max_episode_steps=100, order_enforce=True, autoreset=False, disable_env_checker=False, new_step_api=False, kwargs={'map_name': '4x4'}, namespace=None, name='FrozenLake', version=1), EnvSpec(id='FrozenLake8x8-v1', entry_point='gym.envs.toy_text.frozen_lake:FrozenLakeEnv', reward_threshold=0.85, nondeterministic=False, max_episode_steps=200, order_enforce=True, autoreset=False, disable_env_checker=False, new_step_api=False, kwargs={'map_name': '8x8'}, namespace=None, name='FrozenLake8x8', version=1), EnvSpec(id='CliffWalking-v0', entry_point='gym.envs.toy_text.cliffwalking:CliffWalkingEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=True, autoreset=False, disable_env_checker=False, new_step_api=False, kwargs={}, namespace=None, name='CliffWalking', version=0), EnvSpec(id='Taxi-v3', entry_point='gym.envs.toy_text.taxi:TaxiEnv', reward_threshold=8, nondeterministic=False, max_episode_steps=200, order_enforce=True, autoreset=False, disable_env_checker=False, new_step_api=False, kwargs={}, namespace=None, name='Taxi', version=3), EnvSpec(id='Reacher-v2', entry_point='gym.envs.mujoco:ReacherEnv', reward_threshold=-3.75, nondeterministic=False, max_episode_steps=50, order_enforce=True, autoreset=False, disable_env_checker=False, new_step_api=False, kwargs={}, namespace=None, name='Reacher', version=2), EnvSpec(id='Reacher-v4', entry_point='gym.envs.mujoco.reacher_v4:ReacherEnv', reward_threshold=-3.75, nondeterministic=False, max_episode_steps=50, order_enforce=True, autoreset=False, disable_env_checker=False, new_step_api=False, kwargs={}, namespace=None, name='Reacher', version=4), EnvSpec(id='Pusher-v2', entry_point='gym.envs.mujoco:PusherEnv', reward_threshold=0.0, nondeterministic=False, max_episode_steps=100, order_enforce=True, autoreset=False, disable_env_checker=False, new_step_api=False, kwargs={}, namespace=None, name='Pusher', version=2), EnvSpec(id='Pusher-v4', entry_point='gym.envs.mujoco.pusher_v4:PusherEnv', reward_threshold=0.0, nondeterministic=False, max_episode_steps=100, order_enforce=True, autoreset=False, disable_env_checker=False, new_step_api=False, kwargs={}, namespace=None, name='Pusher', version=4), EnvSpec(id='InvertedPendulum-v2', entry_point='gym.envs.mujoco:InvertedPendulumEnv', reward_threshold=950.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, new_step_api=False, kwargs={}, namespace=None, name='InvertedPendulum', version=2), EnvSpec(id='InvertedPendulum-v4', entry_point='gym.envs.mujoco.inverted_pendulum_v4:InvertedPendulumEnv', reward_threshold=950.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, new_step_api=False, kwargs={}, namespace=None, name='InvertedPendulum', version=4), EnvSpec(id='InvertedDoublePendulum-v2', entry_point='gym.envs.mujoco:InvertedDoublePendulumEnv', reward_threshold=9100.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, new_step_api=False, kwargs={}, namespace=None, name='InvertedDoublePendulum', version=2), EnvSpec(id='InvertedDoublePendulum-v4', entry_point='gym.envs.mujoco.inverted_double_pendulum_v4:InvertedDoublePendulumEnv', reward_threshold=9100.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, new_step_api=False, kwargs={}, namespace=None, name='InvertedDoublePendulum', version=4), EnvSpec(id='HalfCheetah-v2', entry_point='gym.envs.mujoco:HalfCheetahEnv', reward_threshold=4800.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, new_step_api=False, kwargs={}, namespace=None, name='HalfCheetah', version=2), EnvSpec(id='HalfCheetah-v3', entry_point='gym.envs.mujoco.half_cheetah_v3:HalfCheetahEnv', reward_threshold=4800.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, new_step_api=False, kwargs={}, namespace=None, name='HalfCheetah', version=3), EnvSpec(id='HalfCheetah-v4', entry_point='gym.envs.mujoco.half_cheetah_v4:HalfCheetahEnv', reward_threshold=4800.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, new_step_api=False, kwargs={}, namespace=None, name='HalfCheetah', version=4), EnvSpec(id='Hopper-v2', entry_point='gym.envs.mujoco:HopperEnv', reward_threshold=3800.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, new_step_api=False, kwargs={}, namespace=None, name='Hopper', version=2), EnvSpec(id='Hopper-v3', entry_point='gym.envs.mujoco.hopper_v3:HopperEnv', reward_threshold=3800.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, new_step_api=False, kwargs={}, namespace=None, name='Hopper', version=3), EnvSpec(id='Hopper-v4', entry_point='gym.envs.mujoco.hopper_v4:HopperEnv', reward_threshold=3800.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, new_step_api=False, kwargs={}, namespace=None, name='Hopper', version=4), EnvSpec(id='Swimmer-v2', entry_point='gym.envs.mujoco:SwimmerEnv', reward_threshold=360.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, new_step_api=False, kwargs={}, namespace=None, name='Swimmer', version=2), EnvSpec(id='Swimmer-v3', entry_point='gym.envs.mujoco.swimmer_v3:SwimmerEnv', reward_threshold=360.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, new_step_api=False, kwargs={}, namespace=None, name='Swimmer', version=3), EnvSpec(id='Swimmer-v4', entry_point='gym.envs.mujoco.swimmer_v4:SwimmerEnv', reward_threshold=360.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, new_step_api=False, kwargs={}, namespace=None, name='Swimmer', version=4), EnvSpec(id='Walker2d-v2', entry_point='gym.envs.mujoco:Walker2dEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, new_step_api=False, kwargs={}, namespace=None, name='Walker2d', version=2), EnvSpec(id='Walker2d-v3', entry_point='gym.envs.mujoco.walker2d_v3:Walker2dEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, new_step_api=False, kwargs={}, namespace=None, name='Walker2d', version=3), EnvSpec(id='Walker2d-v4', entry_point='gym.envs.mujoco.walker2d_v4:Walker2dEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, new_step_api=False, kwargs={}, namespace=None, name='Walker2d', version=4), EnvSpec(id='Ant-v2', entry_point='gym.envs.mujoco:AntEnv', reward_threshold=6000.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, new_step_api=False, kwargs={}, namespace=None, name='Ant', version=2), EnvSpec(id='Ant-v3', entry_point='gym.envs.mujoco.ant_v3:AntEnv', reward_threshold=6000.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, new_step_api=False, kwargs={}, namespace=None, name='Ant', version=3), EnvSpec(id='Ant-v4', entry_point='gym.envs.mujoco.ant_v4:AntEnv', reward_threshold=6000.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, new_step_api=False, kwargs={}, namespace=None, name='Ant', version=4), EnvSpec(id='Humanoid-v2', entry_point='gym.envs.mujoco:HumanoidEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, new_step_api=False, kwargs={}, namespace=None, name='Humanoid', version=2), EnvSpec(id='Humanoid-v3', entry_point='gym.envs.mujoco.humanoid_v3:HumanoidEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, new_step_api=False, kwargs={}, namespace=None, name='Humanoid', version=3), EnvSpec(id='Humanoid-v4', entry_point='gym.envs.mujoco.humanoid_v4:HumanoidEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, new_step_api=False, kwargs={}, namespace=None, name='Humanoid', version=4), EnvSpec(id='HumanoidStandup-v2', entry_point='gym.envs.mujoco:HumanoidStandupEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, new_step_api=False, kwargs={}, namespace=None, name='HumanoidStandup', version=2), EnvSpec(id='HumanoidStandup-v4', entry_point='gym.envs.mujoco.humanoidstandup_v4:HumanoidStandupEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, new_step_api=False, kwargs={}, namespace=None, name='HumanoidStandup', version=4)])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/envs/registration.py:421: UserWarning: \u001b[33mWARN: The `registry.all` method is deprecated. Please use `registry.values` instead.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_episodes(environment, n_episodes, policy, display=True):\n",
        "    wins = 0\n",
        "    total_reward = 0\n",
        "    for episode in range(n_episodes):\n",
        "        terminated = False\n",
        "        state = environment.reset()\n",
        "        while not terminated:\n",
        "            \n",
        "            if isinstance(policy, str) and policy == 'random':\n",
        "                action = environment.action_space.sample()\n",
        "            else:\n",
        "                action = np.argmax(policy[state])\n",
        "\n",
        "            \n",
        "            next_state, reward, terminated, info = environment.step(action)\n",
        "\n",
        "            \n",
        "            if episode==1 and display:\n",
        "                print(\"Action:\")\n",
        "                environment.render() \n",
        "            \n",
        "            total_reward += reward\n",
        "            \n",
        "            state = next_state\n",
        "            \n",
        "            if terminated and reward == 1.0:\n",
        "                wins += 1\n",
        "    average_reward = total_reward / n_episodes\n",
        "    return wins, total_reward, average_reward"
      ],
      "metadata": {
        "id": "dD_8OvApKp8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "REgUEKe1SZG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a Frozen Lake environment\n",
        "env = gym.make('FrozenLake-v1')\n",
        "# Number of episodes to play\n",
        "n_episodes = 5000\n",
        "# First episode plotted as a sample episode\n",
        "print('First episode:')\n",
        "wins, total_reward, average_reward = run_episodes(env, n_episodes, policy=\"random\", display=False)\n",
        "print('------------------------------------')\n",
        "print('Summary:')\n",
        "print(f'- number of wins over {n_episodes} episodes = {wins}')\n",
        "print(f'- average reward over {n_episodes} episodes = {average_reward}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmY9Bfc5K74N",
        "outputId": "e12f088d-3bf1-43da-d2eb-2d4d0b0a0581"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First episode:\n",
            "------------------------------------\n",
            "Summary:\n",
            "- number of wins over 5000 episodes = 65\n",
            "- average reward over 5000 episodes = 0.013\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pygame \n",
        "\n",
        "import os\n",
        "os.environ['SDL_VIDEODRIVER']='dummy'\n",
        "import pygame\n",
        "pygame.display.set_mode((640,480))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVgHweCqMP-t",
        "outputId": "114dc649-2d5c-4add-ccc2-76106ad9a9e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.9/dist-packages (2.3.0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Surface(640x480x32 SW)>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset()\n",
        "env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcDtos8jQuHt",
        "outputId": "56b51f0e-695c-47e9-a124-f14899cf90b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
            "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Create a Policy Iteration function with the following parameters\n",
        "\n",
        "> •policy: 2D array of a size n(S) x n(A), each cell represents a probability of taking action a in state s.\n",
        "\n",
        "> •environment: Initialized OpenAI gym environment object•discount_factor: MDP discount factor\n",
        "\n",
        "\n",
        ">•theta:  A  threshold  of  a  value  function  change.  Once  the  update  to  value function is below this number\n",
        "\n",
        "\n",
        "> •max_iterations: Maximum number of iterations"
      ],
      "metadata": {
        "id": "1rATLTFzG9dy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_evaluation(policy, environment, discount_factor=1.0, theta=1e-9, max_iterations=1e9):\n",
        "    n_states = environment.observation_space.n\n",
        "    n_actions = environment.action_space.n\n",
        "    # Number of evaluation iterations\n",
        "    evaluation_iterations = 1\n",
        "    # Initialize a value function for each state as zero\n",
        "    V = np.zeros(n_states)\n",
        "    # Repeat until change in value is below the threshold\n",
        "    for i in range(int(max_iterations)):\n",
        "        # Initialize a change of value function as zero\n",
        "        delta = 0\n",
        "        # Iterate though each state\n",
        "        for state in range(n_states):\n",
        "            v = V[state]\n",
        "            v_a = []\n",
        "            for action in range(n_actions):\n",
        "                v_x = 0\n",
        "                for (p, new_state, reward, terminating) in environment.P[state][action]:\n",
        "                    v_x += policy[state][action] * p * (reward + V[new_state])\n",
        "                v_a.append(v_x)\n",
        "\n",
        "                # Update value function\n",
        "            V[state] = max(v_a)\n",
        "                \n",
        "            # Calculate the absolute change of value function\n",
        "            delta = max(delta, np.abs(V[state] - v))\n",
        "            # Update value function\n",
        "            #V[state] = v\n",
        "        evaluation_iterations += 1\n",
        "\n",
        "        # Terminate if value change is insignificant\n",
        "        if delta < theta:\n",
        "            # TODO - check how many iterations\n",
        "            print(f'Policy-evaluation converged at iteration #{evaluation_iterations}')\n",
        "            return V"
      ],
      "metadata": {
        "id": "AF6WPpK5IGe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def one_step_lookahead(environment, state, V, discount_factor):\n",
        "    n_states = environment.observation_space.n\n",
        "    n_actions = environment.action_space.n\n",
        "    action_values = np.zeros(n_actions)\n",
        "    for action in range(n_actions):\n",
        "        for probability, next_state, reward, terminated in environment.P[state][action]:\n",
        "            action_values[action] += probability * (reward + discount_factor * V[next_state])\n",
        "    return action_values"
      ],
      "metadata": {
        "id": "EjO_aasdKCnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_iteration(environment, discount_factor=1.0, max_iterations=1e9):\n",
        "    # Start with a random policy\n",
        "    # num states x num actions / num actions\n",
        "    n_states = environment.observation_space.n\n",
        "    n_actions = environment.action_space.n\n",
        "    policy = np.ones([n_states, n_actions]) / n_actions\n",
        "    \n",
        "    # Initialize counter of evaluated policies\n",
        "    evaluated_policies = 1\n",
        "    # Repeat until convergence or critical number of iterations reached\n",
        "    for i in range(int(max_iterations)):\n",
        "        stable_policy = True\n",
        "        # Evaluate current policy\n",
        "        V = policy_evaluation(policy, environment, discount_factor=discount_factor)\n",
        "        # Go through each state and try to improve actions that were taken (policy Improvement)\n",
        "        for state in range(n_states):\n",
        "            # Choose the best action in a current state under current policy\n",
        "            current_action =  policy[state].tolist().index(max(policy[state]))  \n",
        "            # Look one step ahead and evaluate if current action is optimal\n",
        "            # We will try every possible action in a current state\n",
        "            action_value = one_step_lookahead(environment, state, V, discount_factor)\n",
        "            # Select a better action\n",
        "            best_action = action_value.tolist().index(max(action_value))\n",
        "            # If action didn't change\n",
        "            if current_action != best_action:\n",
        "                stable_policy = True\n",
        "                # Greedy policy update\n",
        "                policy[state] = np.eye(n_actions)[best_action]\n",
        "        evaluated_policies += 1\n",
        "        # If the algorithm converged and policy is not changing anymore, then return final policy and value function\n",
        "        if stable_policy:\n",
        "            return policy, V"
      ],
      "metadata": {
        "id": "oTDm7E18KFWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of episodes to play\n",
        "n_episodes = 1000\n",
        "iteration_name = \"Policy iteration\"\n",
        "iteration_func = policy_iteration\n",
        "# Load a Frozen Lake environment\n",
        "environment = gym.make('FrozenLake-v1')\n",
        "environment.reset()\n",
        "environment.render()\n",
        "\n",
        "# Search for an optimal policy using policy iteration\n",
        "policy, V = iteration_func(environment.env)\n",
        "# Apply best policy to the real environment\n",
        "wins, total_reward, average_reward = run_episodes(environment, n_episodes, policy, False)\n",
        "print(f'{iteration_name}: number of wins over {n_episodes} episodes = {wins}')\n",
        "print(f'{iteration_name}: average reward over {n_episodes} episodes = {average_reward} \\n\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yR9H_tm2Sx7A",
        "outputId": "d50cd80c-98b8-40ff-86d6-39b31751b4f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
            "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy-evaluation converged at iteration #12\n",
            "Policy iteration: number of wins over 1000 episodes = 438\n",
            "Policy iteration: average reward over 1000 episodes = 0.438 \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Create a Value Iteration function with the following parameters\n",
        "\n",
        "> a.environment: Initialized OpenAI gym environment object\n",
        "\n",
        "\n",
        "> b.discount_factor: MDP discount factor\n",
        "\n",
        "\n",
        "> c.theta:  A  threshold  of  a  value  function  change.  Once  the  update  to  value function is below this number\n",
        "\n",
        "\n",
        "> d.max_iterations: Maximum number of iterations\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pMKYHLbbWmi2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def value_iteration(environment, discount_factor=1.0, theta=1e-9, max_iterations=1e9):\n",
        "\n",
        "    n_states = environment.observation_space.n\n",
        "    n_actions = environment.action_space.n\n",
        "    # Initialize state-value function with zeros for each environment state\n",
        "    V = np.zeros(n_states)\n",
        "    for i in range(int(max_iterations)):\n",
        "        # Early stopping condition\n",
        "        delta = 0\n",
        "        # Update each state\n",
        "        for state in range(n_states):\n",
        "            # Do a one-step lookahead to calculate state-action values\n",
        "            action_value = one_step_lookahead(environment, state, V, discount_factor)\n",
        "            # Select best action to perform based on the highest state-action value\n",
        "            best_action_value = max(action_value)\n",
        "            # Calculate change in value\n",
        "            delta = max(delta, np.abs(V[state] - best_action_value))\n",
        "            # Update the value function for current state\n",
        "            V[state] = best_action_value\n",
        "            # Check if we can stop\n",
        "        if delta < theta:\n",
        "            print(f'Value-iteration converged at iteration #{i}.')\n",
        "            break\n",
        "\n",
        "    # Create a deterministic policy using the optimal value function\n",
        "    policy = np.zeros([n_states, n_actions])\n",
        "    for state in range(n_states):\n",
        "        # One step lookahead to find the best action for this state\n",
        "        action_value = one_step_lookahead(environment, state, V, discount_factor)\n",
        "        # Select best action based on the highest state-action value\n",
        "        best_action = action_value.tolist().index(max(action_value))\n",
        "        # Update the policy to perform a better action at a current state\n",
        "        policy[state, best_action] = 1.0\n",
        "    return policy, V"
      ],
      "metadata": {
        "id": "4DH3LY-lKLeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of episodes to play\n",
        "n_episodes = 1000\n",
        "iteration_name = \"Value iteration\"\n",
        "iteration_func = value_iteration\n",
        "# Load a Frozen Lake environment\n",
        "environment = gym.make('FrozenLake-v1') #make environment\n",
        "environment.reset()\n",
        "environment.render()\n",
        "# Search for an optimal policy using policy iteration\n",
        "policy, V = iteration_func(environment.env)\n",
        "# Apply best policy to the real environment\n",
        "wins, total_reward, average_reward = run_episodes(environment, n_episodes, policy, False)\n",
        "print(f'{iteration_name}: number of wins over {n_episodes} episodes = {wins}')\n",
        "print(f'{iteration_name}: average reward over {n_episodes} episodes = {average_reward} \\n\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayzZ44Y-XV5w",
        "outputId": "a88e8226-400d-45c0-95a4-54194ec5c857"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value-iteration converged at iteration #523.\n",
            "Value iteration: number of wins over 1000 episodes = 734\n",
            "Value iteration: average reward over 1000 episodes = 0.734 \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Compare  the number of  wins, average  return  after  1000  episodes andcomment  on which method performed better."
      ],
      "metadata": {
        "id": "_9Gt_yQHXsTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of episodes to play\n",
        "n_episodes = 1000\n",
        "# Functions to find best policy\n",
        "solvers = [('Policy Iteration', policy_iteration),\n",
        "           ('Value Iteration', value_iteration)]\n",
        "for iteration_name, iteration_func in solvers:\n",
        "    # Load a Frozen Lake environment\n",
        "    environment = gym.make('FrozenLake-v1')\n",
        "    # Search for an optimal policy using policy iteration\n",
        "    policy, V = iteration_func(environment.env)\n",
        "    # Apply best policy to the real environment\n",
        "    wins, total_reward, average_reward = run_episodes(environment, n_episodes, policy, False)\n",
        "    print(f'{iteration_name} :: number of wins over {n_episodes} episodes = {wins}')\n",
        "    print(f'{iteration_name} :: average reward over {n_episodes} episodes = {average_reward} \\n\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnoPzeX9XsH_",
        "outputId": "e5639249-c78c-4bb5-fc2b-c5e345606f23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy-evaluation converged at iteration #12\n",
            "Policy Iteration :: number of wins over 1000 episodes = 447\n",
            "Policy Iteration :: average reward over 1000 episodes = 0.447 \n",
            "\n",
            "\n",
            "Value-iteration converged at iteration #523.\n",
            "Value Iteration :: number of wins over 1000 episodes = 764\n",
            "Value Iteration :: average reward over 1000 episodes = 0.764 \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Policy-evaluation converges faster since in the section the value of the states are compared, but in value-iteration the best-action-value is compared, which can have a bigger difference.***"
      ],
      "metadata": {
        "id": "fRQ2-pL3X57f"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FAihmWT1X5tP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}